{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48ef8e6",
   "metadata": {},
   "source": [
    "# Applied Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282b851",
   "metadata": {},
   "source": [
    "In this final section we will cover some applied examples with NumPy. We will cover a few \"from scratch\" examples as well as using the scikit-learn library. Hopefully this will sell you on the idea using arrays and vectorized math can be useful and practical. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fd1c2-0e8b-457d-91f6-7ca5bdc13306",
   "metadata": {},
   "source": [
    "## Linear Regression with Hill Climbing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add45a8a",
   "metadata": {},
   "source": [
    "While this is not the most efficient algorithm available for linear regression, it is a cool exercise nonetheless that teaches concepts applicable to other problems. More specifically, we are going to learn hill climbing. The idea behind hill climbing is to make random adjustments to a solution, and if those adjustments result in an improvement, we keep them. We do this thousands or millions or times until the solution does not improve anymore. \n",
    "\n",
    "A simple linear regression solves for coefficients $ \\beta_0 $ and $ \\beta_1 $ given input data $ x $ and output data $ y $. We try to assess a linear relationship between $ x $ and $ y $ and fit the data accordingly. \n",
    "\n",
    "$$\n",
    "\\Large y = \\Large \\beta_0 + \\Large \\beta_1 x\n",
    "$$\n",
    "\n",
    "Let's import these two columns of data $ x $ and $ y $ off of github and save them to two NumPy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a78f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://bit.ly/2KF29Bd\").values\n",
    "x = data[:,0]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40d3ac",
   "metadata": {},
   "source": [
    "Next we are going to declare the beta coefficients `b` as a NumPy array which will initialize both $ \\beta_0 $ and $ \\beta_1 $ as $ 0.0 $. We will also run our hill climbing for 150,000 iterations (`epochs`) and start our loss at a REALLY high number. This will make more sense shortly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bec92f-fbcb-4dd1-a667-ec97144c40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "b0 = np.array([0.0, 0.0]) \n",
    "\n",
    "epochs = 150000  # The number of iterations to perform\n",
    "n = float(data.shape[0])  # Number of points\n",
    "best_loss = 10000000000000.0  # Initialize with a really large value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d37a2b",
   "metadata": {},
   "source": [
    "We are going to make our objective a loss function, more specifically the sum of squares. For a given line with $ \\beta_0 $ and $ \\beta_1 $, we calculate the differences between the line and the given data points, and then square and sum those differences. This sum of squares is what we want to minimize. For every random adjustment to $ \\beta_0 $ and $ \\beta_1 $, we will calculate the sum of squares and see if it has been reduced. If not, we undo those random adjustments.\n",
    "\n",
    "But how do we randomly adjust $ \\beta_0 $ and $ \\beta_1 $? We can use random values from the normal distribution, with a mean of 0 and standard deviation of 1. This will create a volum of mostly small adjustments near 0, but occasionally we make larger adjustments on the tails in the positive and negative directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    # Randomly adjust \"m\" and \"b\"\n",
    "    random_adj = np.random.normal(loc=0,scale=1,size=2)\n",
    "    b0 += random_adj\n",
    "    \n",
    "    # Calculate loss, which is total sum squared error\n",
    "    new_loss = ((y - (b0[0] + b0[1] * x)) ** 2).sum()\n",
    "    \n",
    "    # If loss has improved, keep new values. Otherwise revert.\n",
    "    if new_loss < best_loss:\n",
    "        print(f\"y = {b0[0]} + {b0[1]}x\")\n",
    "        best_loss = new_loss\n",
    "    else:\n",
    "        b0 -= random_adj\n",
    "        \n",
    "print(f\"y = {b0[0]} + {b0[1]}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2aa34",
   "metadata": {},
   "source": [
    "To quickly see the quality of the fit, let's use matplotlib to see how well the line fit to the data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "# show in chart\n",
    "\n",
    "plt.plot(x, y, 'o') # scatterplot\n",
    "plt.plot(x, b0[1]*x + b0[0]) # line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05b052-7fc3-4578-841e-d4a1cb799566",
   "metadata": {},
   "source": [
    "## Linear Regression with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843685c",
   "metadata": {},
   "source": [
    "While hill climbing can be used to solve a variety of problems, linear regression has some shortcuts using linear algebra methods which we cover [in another course](https://learning.anaconda.cloud/linear-algebra). But for now, we can use scikit-learn and leverage NumPy arrays to pass the data. \n",
    "\n",
    "Below, we read the same CSV from the previous example and extract out the `X` and `y` columns. We then fit a `LinearRegression` and get the coefficients from `coef_` and `intercept_`. However, those will return multidimensional arrays so we flatten them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0e9d2-947c-499e-b2a2-eb045ec6021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"https://bit.ly/2KF29Bd\", delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "x = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "y = df.values[:, -1]\n",
    "\n",
    "# Fit a line to the points\n",
    "fit = LinearRegression().fit(x, y)\n",
    "\n",
    "# m = 1.7867224, b = -16.51923513\n",
    "b1 = fit.coef_.flatten()\n",
    "b0 = fit.intercept_.flatten()\n",
    "print(f\"b1 = {b1}\")\n",
    "print(f\"b0 = {b0}\")\n",
    "\n",
    "# show in chart\n",
    "plt.plot(x, y, 'o') # scatterplot\n",
    "plt.plot(x, b1*x+b0) # line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6dadc-0fd1-4e66-abb8-c6a2ad30c998",
   "metadata": {},
   "source": [
    "## Customer Queue Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b64d96",
   "metadata": {},
   "source": [
    "In this example, we are going to simulate customers walking into a bank, grocery store, etc. where they are processed one-a-time. To keep things simple we will only have a single clerk to process each customer, although you are free to adapt this to account for multiple clerks. The purpose of this simulation is to see whether a line will form and get out-of-hand, and we could theoretically use this to predict average wait times of customers. \n",
    "\n",
    "Recall we studied many probability distributions. The normal distribution might make sense for the customer processing time, assuming the processing time follows a normal distribution. But what about the flow of customers into the store? The exponential distribution will  model how much time lapses between each customer walking in. \n",
    "\n",
    "Let's build the simulation below using the normal distribution and exponential distribution. The customers will take on average 3 minutes to process with a standard deviation of .5 minutes. We will model 20 customers on average arriving every hour, but to be consistent in minutes that is $ 1/3 $ of a customer every minute. We will run the simulation for the first 100 customers. \n",
    "\n",
    "Note this is a little involved but the idea is we are using these two distributions to create a \"realistic\" simulation. Run the simulation and observe its output before you dive into the code itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec202e88-da12-4a5b-8dcf-d089e139d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import normal, exponential\n",
    "\n",
    "np.random.seed(0) # use random seed to run identical \"random\" simulations\n",
    "\n",
    "mean_checkout_time = 3  # minutes\n",
    "std_checkout_time = .5  # minutes\n",
    "mean_arrival_rate = 20 / 60  # customers per minute\n",
    "customer_ct = 100\n",
    "\n",
    "# customer arrival times relative to the previous customer\n",
    "customer_time_betweens = exponential(scale=1/mean_arrival_rate, size=customer_ct+2) # need to add 2 to prevent out-of-index errors\n",
    "\n",
    "# customer arrival times as minutes since start of simulation\n",
    "customer_arrival_times = np.cumsum(customer_time_betweens)\n",
    "\n",
    "# customer checkout times\n",
    "customer_checkout_times = normal(loc=mean_checkout_time, scale=std_checkout_time, size=customer_ct+2) # need to add 2 to prevent out-of-index errors\n",
    "\n",
    "# start time at 0 but jump to first customer arrival, and track whether customer is being processed\n",
    "# and which customers are waiting\n",
    "current_time = customer_arrival_times[0]\n",
    "waiting_customers = []\n",
    "\n",
    "arrived_customer_i = 0\n",
    "processing_customer_i = 0\n",
    "processing_customer_start_time = customer_arrival_times[0]\n",
    "\n",
    "# process customers but stop when all customers have arrived \n",
    "while arrived_customer_i < customer_ct:\n",
    "\n",
    "    # arrival time of processing customer\n",
    "    processing_cust_arr_tm = customer_arrival_times[processing_customer_i]\n",
    "\n",
    "    # scheduled finish time of processing customer\n",
    "    processing_cust_fin_tm = processing_customer_start_time + \\\n",
    "                             customer_checkout_times[processing_customer_i]\n",
    "\n",
    "    # time of next customer arrival\n",
    "    def next_cust_arr_tm(): return customer_arrival_times[arrived_customer_i+1]\n",
    "\n",
    "    # CHECK WHICH EVENT OCCURRED BY MATCHING THE TIMES\n",
    "    next_event_time = None\n",
    "\n",
    "    # if the first customer\n",
    "    if current_time == processing_customer_start_time:\n",
    "        print(f\"{current_time}: CUSTOMER {arrived_customer_i} ARRIVED, NO LINE, PROCESSING IMMEDIATELY\")\n",
    "        next_event_time = np.min([processing_cust_fin_tm, next_cust_arr_tm()])\n",
    "\n",
    "    # if a customer arrives\n",
    "    elif current_time == next_cust_arr_tm():\n",
    "        arrived_customer_i +=1 # increment the arrived customer index\n",
    "\n",
    "        # if there is no queue and the arriving customer is next\n",
    "        if processing_customer_i == arrived_customer_i:\n",
    "            processing_customer_start_time = current_time\n",
    "            processing_cust_fin_tm = processing_customer_start_time + customer_checkout_times[processing_customer_i]\n",
    "\n",
    "            print(f\"{current_time}: CUSTOMER {arrived_customer_i} ARRIVED, NO LINE, PROCESSING IMMEDIATELY\")\n",
    "        # else there is a queue and the customer must wait in line\n",
    "        else:\n",
    "            waiting_customers.append(arrived_customer_i)\n",
    "            print(f\"{current_time}: CUSTOMER {arrived_customer_i} ARRIVED, ADDING TO LINE {waiting_customers}\")\n",
    "\n",
    "        # schedule next event time to be the processing customer finishing or the next customer arrival\n",
    "        next_event_time = np.min([processing_cust_fin_tm, next_cust_arr_tm()])\n",
    "\n",
    "    # if a customer finishes processing\n",
    "    elif current_time == processing_cust_fin_tm:\n",
    "\n",
    "        # if queue is not empty, take customer out of queue\n",
    "        if waiting_customers:\n",
    "            waiting_customers.pop(0)\n",
    "            print(f\"{current_time}: CUSTOMER {processing_customer_i} FINISHED, CUSTOMER {processing_customer_i + 1}\"\n",
    "                  f\" REMOVED FROM LINE {waiting_customers}\")\n",
    "\n",
    "            processing_customer_start_time = current_time\n",
    "\n",
    "            # next event is this customer finishing or the next customer arrival\n",
    "            next_event_time = np.min([processing_customer_start_time + customer_checkout_times[processing_customer_i +1],\n",
    "                                      next_cust_arr_tm()])\n",
    "\n",
    "        else:\n",
    "            # if the queue is empty, wait for next customer \n",
    "            print(f\"{current_time}: CUSTOMER {processing_customer_i} FINISHED, WAITING FOR CUSTOMER {processing_customer_i+1}\")\n",
    "            next_event_time = next_cust_arr_tm()\n",
    "\n",
    "        processing_customer_i += 1 # process next customer\n",
    "\n",
    "    # move forward to next event\n",
    "    current_time = next_event_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae52a9",
   "metadata": {},
   "source": [
    "When you run the simulation above, you will see a line does irrecoverably build up after enough time has passed. This should tell you another teller might be necessary to process customers! You can also experiment with shorter processing times or longer times in-between customers, and you will find there is an ideal balance at some point where the processing keeps up with the queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341f7ff-1658-4a3d-bc63-aaba1052daa5",
   "metadata": {},
   "source": [
    "## Neural Network with scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61221b",
   "metadata": {},
   "source": [
    "Let's look at one final example of using NumPy against scikit-learn. Let's first bring in some imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397085e-d4b4-40ab-8f70-68700fb9add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f6402",
   "metadata": {},
   "source": [
    "Let's bring in the MNIST dataset. Notice each row is an image of a handwritten digit, although this is not quite obvious yet. But we do see each column holds a pixel value for each image/row. The only exception is the last column, which is the label of what digit that image represents between 0 through 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a3ea4-02d5-4e35-9fb6-b318b5cf4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('https://bit.ly/3ilJc2C', compression='zip', delimiter=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2a6a2",
   "metadata": {},
   "source": [
    "These pixel values represent how darkened it is, on a scale between 0 and 255. Let's divide by 255 so it is between 0 and 1. Let's also grab the input columns of pixels as `x`, and the output column of the labels `y`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0592d7b-021e-4881-8506-e321f602b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df.values[:, :-1] / 255.0\n",
    "y = df.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55c28a",
   "metadata": {},
   "source": [
    "In machine learning, it is good practice to set aside part of the data as the test dataset (e.g., 1/3 of the data), and use the remaining data as the training dataset (e.g., 2/3). This way we can later use the test data to evaluate how well the model works on data it has not seen before. The `train_test_split()` will do this for us and return four NumPy arrays serving as the train/test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c111f8-5007-4bb9-9391-e2aa7245bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training and testing data\n",
    "# each class is proportionally represented in both sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y,\n",
    "    test_size=1/3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b08c0",
   "metadata": {},
   "source": [
    "Finally, we will create a classification neural network `MLPClassifier` from scikit-learn. We will pass the `X_train` and `Y_train` data to the model, use 100 hidden nodes in a single hidden layer, and use a logistic activation function for the hidden layer. If you are not familiar with machine learning or neural networks, [there is an Anaconda class teaching that topic](https://learning.anaconda.cloud/getting-started-with-ai-ml). \n",
    "\n",
    "Let's train the model, and then evaluate the accuracy the test data. We can go a step further and create a confusion matrix, which tracks how often each digit was correctly identified, and when they were not what digits they were misclassified as. The confusion matrix itself will be returned as a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c55b57-fcd9-47b7-97b1-31ee66ab7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn = MLPClassifier(solver='sgd',\n",
    "                   hidden_layer_sizes=(100, ),\n",
    "                   activation='logistic',\n",
    "                   max_iter=480,\n",
    "                   learning_rate_init=.1)\n",
    "\n",
    "nn.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Test set score: {nn.score(X_test, Y_test)}\")\n",
    "\n",
    "cf = confusion_matrix(y_true=Y_test, y_pred=nn.predict(X_test))\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a5307",
   "metadata": {},
   "source": [
    "That should give us plenty of examples to ponder. Hopefully you see that NumPy is a building block to perform many tasks and work with many libraries like scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2ea66",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59c381",
   "metadata": {},
   "source": [
    "Complete the code below to perform a linear regression on [this dataset](https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/regression/linear_normal.csv) on Github. The left column is the `x` input variable, and the right column is the `y` output variable. \n",
    "\n",
    "Complete by replacing the question marks `?` below, including extracting the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = r\"https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/regression/linear_normal.csv\"\n",
    "df = pd.read_csv(url, delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "x = ? \n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "y = ?\n",
    "\n",
    "# Fit a line to the points\n",
    "fit = LinearRegression().fit(?, ?)\n",
    "\n",
    "# m = 1.7867224, b = -16.51923513\n",
    "b1 = fit.coef_.?\n",
    "b0 = fit.intercept_.?\n",
    "print(f\"b1 = {b1}\")\n",
    "print(f\"b0 = {b0}\")\n",
    "\n",
    "# show in chart\n",
    "plt.plot(x, y, 'o') # scatterplot\n",
    "plt.plot(x, b1*x+b0) # line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0812529",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = r\"https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/regression/linear_normal.csv\"\n",
    "df = pd.read_csv(url, delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "x = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "y = df.values[:, -1]\n",
    "\n",
    "# Fit a line to the points\n",
    "fit = LinearRegression().fit(x, y)\n",
    "\n",
    "# m = 1.7867224, b = -16.51923513\n",
    "b1 = fit.coef_.flatten()\n",
    "b0 = fit.intercept_.flatten()\n",
    "print(f\"b1 = {b1}\")\n",
    "print(f\"b0 = {b0}\")\n",
    "\n",
    "# show in chart\n",
    "plt.plot(x, y, 'o') # scatterplot\n",
    "plt.plot(x, b1*x+b0) # line\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
